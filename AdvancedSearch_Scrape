from bs4 import BeautifulSoup
import random
import requests
import lxml
import time


def GET_UA():
    uastrings = ["Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36",\
                "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36",\
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25",\
                "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0",\
                "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36",\
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36",\
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.1.17 (KHTML, like Gecko) Version/7.1 Safari/537.85.10",\
                "Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko",\
                "Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0",\
                "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36"\
                ]
    return random.choice(uastrings)

filename = "Advanced Search Scrape.csv"
f = open(filename, "w")
header = "Username: , Email: , Ships: , Characters: \n"
f.write(header)

i = 1
while (i < 100):
    try:
        headers = {'User-Agent': GET_UA()}
        
        if i % 10 == 0:
            print('Page',i, 10)
            time.sleep(30)
        else:
            time.sleep(10)
            print('Page',i, 2)
        
        page = requests.get("https://archiveofourown.org/works/search?commit=Search&page=" + str(i) + "&utf8=%E2%9C%93&work_search%5Bbookmarks_count%5D=&work_search%5Bcharacter_names%5D=&work_search%5Bcomments_count%5D=&work_search%5Bcomplete%5D=&work_search%5Bcreators%5D=&work_search%5Bcrossover%5D=&work_search%5Bfandom_names%5D=&work_search%5Bfreeform_names%5D=&work_search%5Bhits%5D=&work_search%5Bkudos_count%5D=&work_search%5Blanguage_id%5D=&work_search%5Bquery%5D=&work_search%5Brating_ids%5D=&work_search%5Brelationship_names%5D=&work_search%5Brevised_at%5D=&work_search%5Bsingle_chapter%5D=0&work_search%5Bsort_column%5D=revised_at&work_search%5Bsort_direction%5D=desc&work_search%5Btitle%5D=&work_search%5Bword_count%5D=", headers=headers)
        page_html = BeautifulSoup(page.content, "lxml")
        
        works = page_html.find_all("li", {"role" : "article"})

        for work in works:
            if work.find("h4", { "class" : "heading" }) is not None:
                header = work.find("h4", { "class" : "heading" })
                users = header.find_all("a", {"rel" : "author"})
                tags = work.find("ul", {"class" : "tags commas"})

                #tag specifics
                ships = tags.find_all("li", {"class" : "relationships"})
                characters = tags.find_all("li", {"class" : "characters"})
                
                for user in users:
                    link_base = user["href"]
                    link = link_base.replace("/users", "https://archiveofourown.org/users")

                    user_page = requests.get(link)
                    user_page_html = BeautifulSoup(user_page.content, "lxml")
                    navigation = user_page_html.find("ul", {"class" : "navigation actions"})
                    dashboard = navigation.select_one("a")
                    user_prof_base = dashboard["href"] + "/profile"
                    profile_link = user_prof_base.replace("/users" , "https://archiveofourown.org/users")
                    #print("Profile: " + profile_link)

                    profiles = requests.get(profile_link)
                    profile_html = BeautifulSoup(profiles.content, "lxml")
                    home_profile = profile_html.find("div", {"class" : "user home profile"})
                    
                    if home_profile.find("dt", {"class" : "email"}) is not None:
                        email_location = home_profile.find("dd", {"class" : "email"})
                        email = email_location.text 
                        #print("email: " + email )
                        f.write(username + ",")
                        f.write(email + ",")

                        print("Relationships: ")
                        for ship in ships:
                            print(ship.text)
                            f.write(ship_text + "; ")

                        f.write(",")
                        
                        print("Characters: ")
                        for character in characters:
                            print(character.text)
                            f.write(ch_text + "; ")

                        f.write("\n")
        i += 1
    except Exception:
        time.sleep(120)            
        i += 1

